{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16878,
     "status": "ok",
     "timestamp": 1741097284230,
     "user": {
      "displayName": "Massimo",
      "userId": "14178866670900170492"
     },
     "user_tz": -60
    },
    "id": "Y_2hXDIu8bVI",
    "outputId": "47c38ec6-53b5-4c09-d818-82208523e8d4"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "#before running upload the task_1 dataset in your google drive\n",
    "\n",
    "#put the name of your folder (or the relative path)\n",
    "folder_name = \"face_db\"\n",
    "folder_path = f\"/content/drive/My Drive/{folder_name}/\"\n",
    "\n",
    "os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "rfxkywE7e0vK"
   },
   "outputs": [],
   "source": [
    "!ls drive/MyDrive/deepfake_det_task1/development/real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fwd8j03U7FQq"
   },
   "outputs": [],
   "source": [
    "def get_image_paths_and_labels(base_path, label):\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_paths.append((os.path.join(root, file), label))\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "n4kdSvSv7NS2"
   },
   "outputs": [],
   "source": [
    "real_path = os.path.join(folder_path, \"development\", \"real\")\n",
    "fake_path = os.path.join(folder_path, \"development\", \"fake\")\n",
    "real_test_path = os.path.join(folder_path, \"evaluation\", \"real\")\n",
    "fake_test_path = os.path.join(folder_path, \"evaluation\", \"fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mvKOkC5Q7H-G"
   },
   "outputs": [],
   "source": [
    "#extract paths and labels for all images of training data\n",
    "real_images = get_image_paths_and_labels(real_path, label=0)  # 0 for real\n",
    "fake_images = get_image_paths_and_labels(fake_path, label=1)  # 1 for fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cyFoVkCp7d_i"
   },
   "outputs": [],
   "source": [
    "# compare number of images per set\n",
    "print(len(real_images))\n",
    "print(len(fake_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "lH2ZR836UKip"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class DeepFakeDataset(Dataset):\n",
    "    def __init__(self, real_dir, fake_dir, transform=None, target_size=(224, 224)):\n",
    "        # Get image paths and labels\n",
    "        self.real_images = self._get_image_paths(real_dir)\n",
    "        self.fake_images = self._get_image_paths(fake_dir)\n",
    "        self.images = self.real_images + self.fake_images\n",
    "        self.labels = [0] * len(self.real_images) + [1] * len(self.fake_images)\n",
    "\n",
    "        # Create pairs of image paths and labels\n",
    "        self.data = list(zip(self.images, self.labels))\n",
    "        # Shuffle the dataset\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        # Load the Haar cascade classifier for face detection\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "        # Pre-filter the dataset\n",
    "        self._prefilter_dataset()\n",
    "\n",
    "    def _prefilter_dataset(self):\n",
    "        \"\"\"Pre-filter the dataset to remove images without faces\"\"\"\n",
    "        filtered_data = []\n",
    "        print(f\"Pre-filtering dataset of {len(self.data)} images...\")\n",
    "\n",
    "        for img_path, label in self.data:\n",
    "            try:\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    print(f\"Failed to read image {img_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert image to grayscale for face detection\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Detect faces\n",
    "                faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "                if len(faces) > 0:\n",
    "                    filtered_data.append((img_path, label))\n",
    "                else:\n",
    "                    print(f\"No face detected in image {img_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_path}: {str(e)}\")\n",
    "\n",
    "        print(f\"Filtered dataset contains {len(filtered_data)} images\")\n",
    "        if len(filtered_data) == 0:\n",
    "            print(\"WARNING: No images with faces found! Using original dataset.\")\n",
    "            self.data = self.data  # Keep original data if no faces found\n",
    "        else:\n",
    "            self.data = filtered_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "\n",
    "        try:\n",
    "            # Read the image\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                return self._create_dummy_item(label, img_path)\n",
    "\n",
    "            # Save a copy of the original image for display purposes\n",
    "            original_image = image.copy()\n",
    "\n",
    "            # Convert image to grayscale for face detection\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect faces\n",
    "            faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                # Use the largest face detected\n",
    "                largest_face = max(faces, key=lambda face: face[2] * face[3])\n",
    "                (x, y, w, h) = largest_face\n",
    "\n",
    "                # Expand the face bounding box slightly for better results\n",
    "                padding = int(0.2 * max(w, h))  # 20% padding\n",
    "                x_start = max(0, x - padding)\n",
    "                y_start = max(0, y - padding)\n",
    "                x_end = min(image.shape[1], x + w + padding)\n",
    "                y_end = min(image.shape[0], y + h + padding)\n",
    "\n",
    "                # Crop the image to the face region\n",
    "                face_image = image[y_start:y_end, x_start:x_end]\n",
    "            else:\n",
    "                # This should not happen due to pre-filtering, but just in case\n",
    "                face_image = image\n",
    "\n",
    "            # Convert BGR to RGB (since OpenCV uses BGR by default)\n",
    "            face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Resize the image to the target size\n",
    "            face_image = cv2.resize(face_image, self.target_size)\n",
    "\n",
    "            # Convert to PIL Image for transformations\n",
    "            face_image = Image.fromarray(face_image)\n",
    "\n",
    "            # Apply the transformations (ToTensor, Normalize, etc.)\n",
    "            if self.transform:\n",
    "                face_image = self.transform(face_image)\n",
    "\n",
    "            # Convert original image to RGB for consistent return\n",
    "            original_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "            original_rgb = cv2.resize(original_rgb, self.target_size)\n",
    "            original_rgb = Image.fromarray(original_rgb)\n",
    "\n",
    "            if self.transform:\n",
    "                # Apply same transform to original image to ensure consistent tensor shapes\n",
    "                original_tensor = self.transform(original_rgb)\n",
    "            else:\n",
    "                # If no transform, convert to tensor manually\n",
    "                original_tensor = torch.from_numpy(np.array(original_rgb).transpose((2, 0, 1)) / 255.0).float()\n",
    "\n",
    "            return face_image, label, original_tensor, img_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {str(e)}\")\n",
    "            return self._create_dummy_item(label, img_path)\n",
    "\n",
    "    def _create_dummy_item(self, label, img_path):\n",
    "        \"\"\"Create a dummy item with the correct dimensions\"\"\"\n",
    "        dummy_image = np.zeros((self.target_size[0], self.target_size[1], 3), dtype=np.uint8)\n",
    "        dummy_pil = Image.fromarray(dummy_image)\n",
    "\n",
    "        if self.transform:\n",
    "            dummy_tensor = self.transform(dummy_pil)\n",
    "        else:\n",
    "            dummy_tensor = torch.from_numpy(dummy_image.transpose((2, 0, 1)) / 255.0).float()\n",
    "\n",
    "        return dummy_tensor, label, dummy_tensor, img_path\n",
    "\n",
    "    def _get_image_paths(self, dir_path):\n",
    "        image_paths = []\n",
    "        for root, _, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_paths.append(os.path.join(root, file))\n",
    "        return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "panhOelJ0ZIU"
   },
   "outputs": [],
   "source": [
    "!pip install dlib\n",
    "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "niFRFrArD7vx"
   },
   "outputs": [],
   "source": [
    "import dlib\n",
    "\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"  # Download from dlib\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Rg_uJdS5PdBk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oSIhM8wxXTFo"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class DeepFakeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, real_dir, fake_dir, transform=None, target_size=(224, 224)):\n",
    "        self.real_images = self._get_image_paths(real_dir)\n",
    "        self.fake_images = self._get_image_paths(fake_dir)\n",
    "        self.images = self.real_images + self.fake_images\n",
    "        self.labels = [0] * len(self.real_images) + [1] * len(self.fake_images)\n",
    "\n",
    "        self.data = list(zip(self.images, self.labels))\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self._prefilter_dataset()\n",
    "\n",
    "    def _prefilter_dataset(self):\n",
    "        filtered_data = []\n",
    "        print(f\"Pre-filtering dataset of {len(self.data)} images...\")\n",
    "        for img_path, label in self.data:\n",
    "            try:\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    continue\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "                if len(faces) > 0:\n",
    "                    filtered_data.append((img_path, label))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_path}: {str(e)}\")\n",
    "        self.data = filtered_data if filtered_data else self.data\n",
    "\n",
    "    def _align_face2(self, image, face):\n",
    "      \"\"\"\n",
    "      Aligns the face using dlib's 68-point facial landmarks.\n",
    "      \"\"\"\n",
    "      gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "      # Get facial landmarks\n",
    "      landmarks = self.predictor(gray, face)\n",
    "\n",
    "      # Extract eye centers\n",
    "      left_eye = np.mean([(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)], axis=0)\n",
    "      right_eye = np.mean([(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)], axis=0)\n",
    "\n",
    "      # Calculate rotation angle\n",
    "      dx = right_eye[0] - left_eye[0]\n",
    "      dy = right_eye[1] - left_eye[1]\n",
    "      angle = np.degrees(np.arctan2(dy, dx))\n",
    "\n",
    "      # Define the center of the face\n",
    "      center = (face.left() + face.width() // 2, face.top() + face.height() // 2)\n",
    "\n",
    "      # Rotate the image to align the eyes horizontally\n",
    "      M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "      aligned_image = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]), flags=cv2.INTER_CUBIC)\n",
    "\n",
    "      # Crop the aligned face\n",
    "      aligned_face = aligned_image[face.top():face.bottom(), face.left():face.right()]\n",
    "\n",
    "      return aligned_face\n",
    "\n",
    "    def _align_face(self, image, face):\n",
    "      (x, y, w, h) = face\n",
    "\n",
    "      # Don't process faces that are too small\n",
    "      if w < 50 or h < 50:\n",
    "          return image[y:y+h, x:x+w]\n",
    "\n",
    "      gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "      try:\n",
    "          eyes = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "          roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "          # Detect eyes with stricter parameters\n",
    "          detected_eyes = eyes.detectMultiScale(roi_gray,\n",
    "                                              scaleFactor=1.1,\n",
    "                                              minNeighbors=5,\n",
    "                                              minSize=(20, 20))\n",
    "\n",
    "          # Create a copy of the original image\n",
    "          image_copy = image.copy()\n",
    "\n",
    "          if len(detected_eyes) >= 2:\n",
    "              # Filter eyes by vertical position (eyes should be in the upper half of the face)\n",
    "              upper_eyes = [eye for eye in detected_eyes if eye[1] < h/2]\n",
    "\n",
    "              if len(upper_eyes) >= 2:\n",
    "                  # Sort eyes by x-coordinate to get left and right eyes\n",
    "                  eye_centers = sorted(upper_eyes, key=lambda ex: ex[0])[:2]\n",
    "\n",
    "                  # Calculate eye centers in the original image coordinates\n",
    "                  left_eye_center = (x + eye_centers[0][0] + eye_centers[0][2]//2,\n",
    "                                    y + eye_centers[0][1] + eye_centers[0][3]//2)\n",
    "                  right_eye_center = (x + eye_centers[1][0] + eye_centers[1][2]//2,\n",
    "                                    y + eye_centers[1][1] + eye_centers[1][3]//2)\n",
    "\n",
    "                  # Calculate angle for horizontal alignment\n",
    "                  dx = right_eye_center[0] - left_eye_center[0]\n",
    "                  dy = right_eye_center[1] - left_eye_center[1]\n",
    "                  angle = np.degrees(np.arctan2(dy, dx))\n",
    "\n",
    "                  # Limit rotation angle to prevent excessive rotation\n",
    "                  if abs(angle) > 30:\n",
    "                      print(f\"Excessive rotation angle {angle} detected, limiting to ±30 degrees\")\n",
    "                      angle = np.sign(angle) * 30\n",
    "\n",
    "                  # Ensure center is properly formatted as a tuple of floats\n",
    "                  center = (float(x + w//2), float(y + h//2))\n",
    "\n",
    "                  try:\n",
    "                      # Apply rotation to the entire image\n",
    "                      M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "                      rotated_image = cv2.warpAffine(image_copy, M, (image.shape[1], image.shape[0]),\n",
    "                                                    flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "                      # Extract and return the aligned face from the rotated image\n",
    "                      return rotated_image[y:y+h, x:x+w]\n",
    "                  except Exception as e:\n",
    "                      print(f\"Error during rotation: {str(e)}\")\n",
    "                      return image[y:y+h, x:x+w]  # Return original cropped face\n",
    "\n",
    "      except Exception as e:\n",
    "          print(f\"Error during eye detection: {str(e)}\")\n",
    "\n",
    "      # If any step fails, return original face crop\n",
    "      return image[y:y+h, x:x+w]\n",
    "\n",
    "    def __extract_landmarks__(self, image):\n",
    "      landmarks = predictor(gray, face)\n",
    "      # Convert to NumPy array\n",
    "      points = np.array([[p.x, p.y] for p in landmarks.parts()], dtype=np.int32)\n",
    "\n",
    "      return points\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        try:\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                return self._create_dummy_item(label, img_path)\n",
    "\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                face = max(faces, key=lambda f: f[2] * f[3])\n",
    "                face_image = self._align_face2(image, face)\n",
    "            else:\n",
    "                face_image = image\n",
    "\n",
    "            face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "            face_image = cv2.resize(face_image, self.target_size)\n",
    "            face_image = Image.fromarray(face_image)\n",
    "\n",
    "            if self.transform:\n",
    "                face_image = self.transform(face_image)\n",
    "\n",
    "            original_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            original_rgb = cv2.resize(original_rgb, self.target_size)\n",
    "            original_rgb = Image.fromarray(original_rgb)\n",
    "            original_tensor = self.transform(original_rgb) if self.transform else torch.from_numpy(np.array(original_rgb).transpose((2, 0, 1)) / 255.0).float()\n",
    "\n",
    "            return face_image, label, original_tensor, img_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {str(e)}\")\n",
    "            return self._create_dummy_item(label, img_path)\n",
    "\n",
    "    def _create_dummy_item(self, label, img_path):\n",
    "        dummy_image = np.zeros((self.target_size[0], self.target_size[1], 3), dtype=np.uint8)\n",
    "        dummy_pil = Image.fromarray(dummy_image)\n",
    "        dummy_tensor = self.transform(dummy_pil) if self.transform else torch.from_numpy(dummy_image.transpose((2, 0, 1)) / 255.0).float()\n",
    "        return dummy_tensor, label, dummy_tensor, img_path\n",
    "\n",
    "    def _get_image_paths(self, dir_path):\n",
    "        image_paths = []\n",
    "        for root, _, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_paths.append(os.path.join(root, file))\n",
    "        return image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FUv1nqyxCaXV"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "\n",
    "class EdgeEnhanceTransform:\n",
    "    def __call__(self, img):\n",
    "        return img.filter(ImageFilter.EDGE_ENHANCE)  # Apply edge enhancement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KNCekmzO8i2E"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224 (optional, if not done earlier)\n",
    "    EdgeEnhanceTransform(),  # Apply edge enhancement\n",
    "    transforms.ToTensor(),  # Convert to tensor (also scales values to [0, 1])\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OEHF0Ozk2EY7"
   },
   "outputs": [],
   "source": [
    "dataset = DeepFakeDataset(real_dir=real_path, fake_dir=fake_path, transform=transform)\n",
    "\n",
    "# Select an example image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "r03Zs9VupYWJ"
   },
   "outputs": [],
   "source": [
    "test_dataset = DeepFakeDataset(real_dir=real_test_path, fake_dir = fake_test_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "n4eFeGIMCoAM"
   },
   "outputs": [],
   "source": [
    "a,b,c,d = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8bj9Qa5ICcxZ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "def visualize_normalized_image(dataset, num_samples=5):\n",
    "    \"\"\"Visualizes images after normalization.\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, num_samples * 4))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        face_tensor, label, original_tensor, img_path = dataset[i]\n",
    "\n",
    "        # Convert tensors to numpy arrays for display\n",
    "        face_image = face_tensor.permute(1, 2, 0).cpu().numpy()  # CHW → HWC\n",
    "        original_image = original_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        # Scale from [-1,1] to [0,1] (ONLY for visualization)\n",
    "        face_image = (face_image - face_image.min()) / (face_image.max() - face_image.min())\n",
    "        original_image = (original_image - original_image.min()) / (original_image.max() - original_image.min())\n",
    "\n",
    "        # Plot images\n",
    "        axes[i, 0].imshow(original_image)\n",
    "        axes[i, 0].set_title(f\"Original Image (After Normalization)\\n{img_path.split('/')[-1]}\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(face_image)\n",
    "        axes[i, 1].set_title(\"Detected Face (After Normalization)\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "N0P50LdiFp8G"
   },
   "outputs": [],
   "source": [
    "visualize_normalized_image(test_dataset, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3h26t5XnVwIO"
   },
   "outputs": [],
   "source": [
    "visualize_normalized_image(dataset, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6Hhnde3b8whf"
   },
   "outputs": [],
   "source": [
    "# Split into train/val datasets\n",
    "from torch.utils.data import random_split\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#val_size = len(dataset) - train_size\n",
    "#train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Total images: {len(dataset) + len(test_dataset)}, Train: {len(dataset)}, Validation: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "uONfzP1dRowJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Rx2PK4QY8vB-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Training loop with early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None,\n",
    "                num_epochs=10, device='cuda', patience=5, save_path='best_model.pth'):\n",
    "\n",
    "    # Move model to device\n",
    "\n",
    "\n",
    "    # Initialize variables for early stopping\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    # Track metrics\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "\n",
    "    # Time tracking\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data batches\n",
    "\n",
    "            for inputs, labels, original_imgs, img_paths in tqdm(dataloader, desc=f\"{phase}\"):\n",
    "                # Our dataset returns 4 items, but we only need the first two for training\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)  # Model outputs probabilities (sigmoid applied)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Convert outputs to binary predictions (0 or 1)\n",
    "                    preds = torch.round(outputs)  # Sigmoid threshold at 0.5\n",
    "\n",
    "                    # Backward pass and optimization only in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Accumulate loss and accuracy\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "\n",
    "            # Compute epoch loss and accuracy\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Store history\n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item())\n",
    "\n",
    "                # Update learning rate scheduler\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step(epoch_loss)\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "\n",
    "                # Save best model based on validation accuracy\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    no_improve_epochs = 0\n",
    "\n",
    "                    # Save best model\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "                    print(f\"Saved best model with accuracy: {best_acc:.4f}\")\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "        print()\n",
    "\n",
    "        # Early stopping\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    # Training complete\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Function to visualize results\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n",
    "    plt.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize predictions with sample images\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_misclassified(model, test_loader, device, num_samples=10, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _, img_paths in test_loader:  # Removed original_imgs\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(inputs.size(0)):\n",
    "                if count >= num_samples:\n",
    "                    break\n",
    "\n",
    "                if preds[i] != labels[i]:  # Only show misclassified samples\n",
    "                    # Get probability of predicted class\n",
    "                    probabilities = torch.nn.functional.softmax(outputs[i], dim=0)\n",
    "                    pred_prob = probabilities[preds[i]].item()\n",
    "\n",
    "                    # Convert tensor to numpy for visualization\n",
    "                    img = inputs[i].cpu().numpy().transpose((1, 2, 0))\n",
    "                    img = img * np.array(std) + np.array(mean)  # Unnormalize\n",
    "                    img = np.clip(img, 0, 1)  # Ensure values are in valid range\n",
    "\n",
    "                    axes[count].imshow(img)\n",
    "\n",
    "                    title = f\"Pred: {'Fake' if preds[i] else 'Real'} ({pred_prob:.2f})\\nTrue: {'Fake' if labels[i] else 'Real'}\"\n",
    "                    axes[count].set_title(title, color='red')  # Always red for misclassified\n",
    "                    axes[count].axis('off')\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example of how to use this code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gqdpZL355F3z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "UeIzpVJh8pt6"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 16\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-4\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "patience = 5  # For early stopping\n",
    "\n",
    "model = DeepFakeDetector()\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "#model = models.resnet18()\n",
    "#model.fc = nn.Linear(model.fc.in_features, 2)  # Adjust for your output classes\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model, history = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler=None,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    patience=patience,\n",
    "    save_path='best_deepfake_model.pth'\n",
    ")\n",
    "\n",
    "# Visualize training history\n",
    "plot_training_history(history)\n",
    "\n",
    "# Visualize some predictions\n",
    "visualize_misclassified(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "nh34uPYWQVoo"
   },
   "outputs": [],
   "source": [
    "visualize_misclassified(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yblqrXzJF_Ly"
   },
   "outputs": [],
   "source": [
    "#ciao caro\n",
    "#extract the testing data\n",
    "\n",
    "#create the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-RdiIJJv6Kit"
   },
   "outputs": [],
   "source": [
    "#create a CNN with three layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NJahJoVKF_iL"
   },
   "outputs": [],
   "source": [
    "#massimo"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
